{
    "original": "Tensor principal component analysis (PCA) is a pivotal tool for analyzing intricate multi-dimensional data that exhibits multiway structure. Traditional PCA methodologies fall short when handling data structures with nonlinearity, non-Gaussian distribution, or reliance on multiple factors. Tensor PCA extends the conventional linear PCA by conceptualizing data as an order-d tensor, thus accommodating nonlinearity and interactive attributes. Despite its efficacy, the computational complexity of tensor PCA is notable, arising from the curse of dimensionality and the non-convex nature of tensor factorization. In response, a novel technique termed sum-of-squares tensor PCA (SOSTPCA) is introduced in this study, leveraging sum-of-squares (SoS) proofs. SoS serves as an efficient optimization method converting polynomial optimization dilemmas into a series of semidefinite programs (SDPs). Building upon this foundation, SOSTPCA formulates tensor PCA as a non-convex polynomial optimization predicament, securing optimal convergence through an SoS relaxation followed by an SDP. Highlighted advantages of SOSTPCA include global optimality, scalability for high-dimensional tensors of varying size and rank, and robustness against noise, outliers, and missing data. Demonstrated effectiveness across synthetic and real-world datasets, encompassing image analysis and neuroscience data, affirms SOSTPCA's superiority in accuracy, efficiency, and robustness over existing tensor PCA methodologies. Theoretical assurances provided underscore the efficacy and complexity of SOSTPCA, heralding a new era of tensor PCA and polynomial optimization driven by SoS proofs.",
    "sampled": "Tensor principal component analysis (PCA) is a pivotal tool for analyzing intricate multi-dimensional data that exhibits multiway structure. Traditional PCA methodologies fall short when handling data structures with nonlinearity, non-Gaussian distribution, or reliance on multiple factors. Tensor PCA extends the conventional linear PCA by conceptualizing data as an order-d tensor, thus accommodating nonlinearity and interactive attributes. Despite its efficacy, the computational complexity of tensor PCA is notable, arising from the curse of dimensionality and the non-convex nature of tensor factorization. In response, a novel technique termed sum-of-squares tensor PCA (SOSTPCA) is introduced in this study, leveraging sum-of-squares (SoS) proofs. SoS serves as an efficient optimization method converting polynomial optimization dilemmas into a series of semidefinite programs (SDPs). Building upon this foundation, SOSTPCA formulates tensor PCA as a non-convex polynomial optimization predicament, securing optimal convergence through an SoS relaxation followed by an SDP. Highlighted advantages of SOSTPCA include global optimality, scalability for high-dimensional tensors of varying size and rank, and robustness against noise, outliers, and missing data. Demonstrated effectiveness across synthetic and real-world datasets, encompassing image analysis and neuroscience data, affirms SOSTPCA's superiority in accuracy, efficiency, and robustness over existing tensor PCA methodologies. Theoretical assurances provided underscore the efficacy and complexity of SOSTPCA, heralding a new era of tensor PCA and polynomial optimization driven by SoS proofs.",
    "replacement_keys": [],
    "original_crit": 0.0013744288589805365,
    "sampled_crit": 0.0013744288589805365,
    "original_llm_likelihood": 0.0013744288589805365,
    "sampled_llm_likelihood": 0.0013744288589805365
}