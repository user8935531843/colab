{
    "original": "Federated learning involves collaborative training of a deep learning model across multiple local devices without sharing raw data. Current federated training methods typically aim to learn a global model by aggregating local model parameters, leading to high communication costs and performance degradation due to non-iid data distribution and class imbalances among devices, particularly in extreme classification scenarios like user recommendation. To address these challenges, we introduce federated multiple label hashing (FedMLH), utilizing label hashing to reduce model size and communication overhead while enhancing accuracy and convergence rates in federated extreme classification tasks compared to traditional federated averaging algorithms. FedMLH demonstrates promising results with up to 3.40X decrease in model size, 18.75X decrease in communication costs, up to 35.5% relative accuracy improvement, and up to 5.5X faster convergence rate, offering a more efficient and effective solution for real-world federated learning applications.",
    "sampled": "Federated learning involves collaborative training of a deep learning model across multiple local devices without sharing raw data. Current federated training methods typically aim to learn a global model by aggregating local model parameters, leading to high communication costs and performance degradation due to non-iid data distribution and class imbalances among devices, particularly in extreme classification scenarios like user recommendation. To address these challenges, we introduce federated multiple multi-label hashing (FedMLH), utilizing label hashing to reduce model size and communication overhead while enhancing accuracy and convergence rates in federated extreme classification tasks compared to traditional federated averaging algorithms. FedMLH demonstrates promising results with up to 3.40X decrease in model size, 18.75X decrease in communication costs, up to 35.5% relative accuracy improvement, and up to 5.5X faster convergence rate, offering a more efficient and effective solution for real-world federated learning applications.",
    "replacement_keys": [
        68
    ],
    "original_crit": 0.0005649678641930223,
    "sampled_crit": 0.0005306119564920664,
    "original_llm_likelihood": 0.0005649678641930223,
    "sampled_llm_likelihood": 0.0005306119564920664
}