{
    "original": "Speech is a powerful form of communication that conveys a wealth of information, aiding in the effective transmission of the speaker's thoughts. However, the intricate processing of acoustic features has often led to the neglect of phoneme or word posterior probability in natural language understanding. Recent advancements in spoken language understanding (SLU) have embraced end-to-end structures that retain uncertainty information, mitigating the impact of errors in speech recognition and ensuring computational efficiency. Leveraging the knowledge gleaned from extensive pre-trained language models (LMs), we advocate for the integration of transformative text LM insights into SLU modules facing data scarcity, utilizing cross-modal distillation techniques. Our approach is validated through performance analysis on Fluent Speech Command, an English SLU benchmark, confirming the feasibility of transferring knowledge from LM's top layer to a speech-based module. This integration aims to create a synergistic relationship between abstracted speech and semantic representation, enhancing speech comprehension efficacy.",
    "sampled": "Speech is a powerful form of communication that conveys a wealth of information, aiding in the effective transmission of the speaker's thoughts. However, the intricate processing of acoustic features has often led to the neglect of phoneme or word posteriority probability in natural language understanding. Recent advancements in spoken language understanding (SLU) have embraced end-to-end structures that retain uncertainty information, mitigating the impact of errors in speech recognition and ensuring computational efficiency. Leveraging the knowledge gleaned from extensive pre-trained language models (LMs), we advocate for the integration of transformative text LM insights into SLU modules facing data scarcity, utilizing cross-modal distillation techniques. Our approach is validated through performance analysis on Fluent Speech Command, an English SLU benchmark, confirming the feasibility of transferring knowledge from LM's top layer to a speech-based module. This integration aims to create a synergistic relationship between abstracted speech and semantic representation, enhancing speech comprehension efficacy.",
    "replacement_keys": [
        39
    ],
    "original_crit": 0.01881331205368042,
    "sampled_crit": 0.01779431849718094,
    "original_llm_likelihood": 0.01881331205368042,
    "sampled_llm_likelihood": 0.01779431849718094
}