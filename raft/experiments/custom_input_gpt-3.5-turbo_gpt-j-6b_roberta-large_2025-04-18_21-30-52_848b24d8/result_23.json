{
    "original": "This study explores innovative neural network architectures aimed at enhancing the representation of long-term dependencies in sequential data. The research proposes utilizing an increased number of memory units in recurrent neural networks (RNNs) to retain a larger history of previous states, feeding this information recursively to the hidden layers through distinct weighted pathways. By expanding upon the traditional recurrent structure of RNNs, the models benefit from an improved mechanism for short-term memory, facilitating the learning of long-term dependencies within sequences. Termed higher order RNNs (HORNNs), these structures draw a parallel to digital filters in signal processing. Similar to RNNs, HORNNs can be optimized through the back-propagation through time technique. This approach holds promise for enhancing a wide range of sequence modeling tasks. Experimental assessments, particularly focused on language modeling tasks utilizing the Penn Treebank (PTB) and English text8 datasets, have demonstrated that the proposed HORNN architecture achieves superior performance compared to regular RNNs and even popular LSTM models.",
    "sampled": "This study explores innovative neural network architectures aimed at enhancing the representation of long-term dependencies in sequential data. The research proposes utilizing an increased number of memory units in recurrent neural networks (RNNs) to retain a larger history of previous states, feeding this information recursively to the hidden layers through distinct weighted pathways. By expanding upon the traditional recurrent structure of RNNs, the models benefit from an improved mechanism for short-term memory, facilitating the learning of long-term dependencies within sequences. Termed increased order RNNs (HORNNs), these structures draw a parallel to digital filters in signal processing. Similar to RNNs, HORNNs can be optimized through the back-propagation through time technique. This approach retains promise for enhancing a wide range of sequence modeling tasks. Experimental assessments, particularly focused on language modeling tasks utilizing the Penn Treebank (PTB) and English text8 datasets, have demonstrated that the proposed HORNN architecture achieves superior performance compared to regular RNNs and even popular LSTM models.",
    "replacement_keys": [
        81,
        111
    ],
    "original_crit": 0.0009650578140281141,
    "sampled_crit": 0.0008378265774808824,
    "original_llm_likelihood": 0.0009650578140281141,
    "sampled_llm_likelihood": 0.0008378265774808824
}