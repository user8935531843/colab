{
    "original": "In this study, we introduce a new method for on-the-fly deep reinforcement learning (DRL) in unmanned aerial vehicles (UAVs), enabling them to independently explore and navigate through intricate outdoor settings. Our approach merges Proximal Policy Optimization (PPO) and Deep Q-Networks (DQNs) DRL techniques to establish a comprehensive learning framework for UAVs. It encompasses processing raw visual inputs from onboard sensors using a cutting-edge convolutional neural network (CNN) to derive high-level features. These features are then inputted into PPO and DQNs for real-time decision-making. By conducting thorough experiments in a realistic aerial simulation environment, we showcase the efficacy of our proposed method in facilitating precise and efficient navigation and exploration tasks for UAVs. This innovative approach paves the way for the creation of dependable and adaptive autonomous UAV systems capable of functioning effectively in demanding and intricate outdoor environments.",
    "sampled": "In this study, we introduce a new method for on-the-fly deep reinforcement learning (DRL) in drones aerial vehicles (UAVs), enabling them to independently explore and navigate through intricately outdoor settings. Our approach consolidates Proximal Policy Optimization (PPO) and Deep Q-Networks (DQNs) DRL techniques to establish a comprehensive learning framework for UAVs. It encompasses processing raw visual inputs from onboard sensors using a cutting-edge convolutional neural network (CNN) to derive high-level features. These features are then inputted into PPO and DQNs for real-time decision-making. By conducting thorough experiments in a realistic aerial simulation environment, we showcase the efficacy of our proposed method in facilitating precise and efficient navigation and exploration tasks for UAVs. This innovative approach paves the way for the creation of dependable and adaptive autonomous UAV systems capable of functioning effectively in demanding and intricate outdoor environments.",
    "replacement_keys": [
        15,
        27,
        32
    ],
    "original_crit": 0.059427306056022644,
    "sampled_crit": 0.0013424641219899058,
    "original_llm_likelihood": 0.059427306056022644,
    "sampled_llm_likelihood": 0.0013424641219899058
}