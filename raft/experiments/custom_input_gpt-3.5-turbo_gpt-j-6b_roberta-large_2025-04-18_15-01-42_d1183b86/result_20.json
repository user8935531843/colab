{
    "original": "This study explores novel neural network architectures aimed at enhancing the modeling of long-term dependencies in sequential data. The researchers propose the integration of additional memory units in recurrent neural networks (RNNs) to retain more historical states, which are recurrently fed back to the hidden layers through weighted paths. By extending the conventional recurrent structure in RNNs, the models are equipped with a more efficient short-term memory mechanism to capture long-term dependencies in sequences. Referred to as higher order RNNs (HORNNs), these structures, similar to RNNs, can be trained using the back-propagation through time technique and are adaptable to a range of sequence modeling tasks. The study focuses on exploring HORNNs for the language modeling task using the Penn Treebank (PTB) and English text8 datasets. The experimental results demonstrate that the proposed HORNNs achieve superior performance on both datasets, surpassing regular RNNs and popular LSTMs in performance.",
    "sampled": "This study explores novel brain network architectures aimed at enhancing the modeling of long-term dependencies in sequential data. The scientists propose the integration of extra memory units in recurrent neural networks (RNNs) to retain more historical states, which are recurrently fed back to the hidden layers through weighted routes. By extending the conventional recurrent structure in RNNs, the models are equipped with a more efficient short-term memory mechanism to capture long-term dependencies in sequences. Referred to as higher order RNNs (HORNNs), these structures, similar to RNNs, can be trained using the back-propagation through time technique and are adaptable to a range of sequence modeling tasks. The study focuses on exploring HORNNs for the language modeling task using the Penn Treebank (PTB) and English text datasets. The experimental results demonstrate that the proposed HORNNs achieve superior performance on both datasets, surpassing regular RNNs and popular LSTMs in performance.",
    "replacement_keys": [
        4,
        19,
        24,
        48,
        123
    ],
    "original_crit": 0.001974746584892273,
    "sampled_crit": 0.0014434955082833767,
    "original_llm_likelihood": 0.001974746584892273,
    "sampled_llm_likelihood": 0.0014434955082833767
}